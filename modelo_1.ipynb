{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "j_Gq3qAbN1kQ",
        "outputId": "ff420fca-7158-4218-d4a7-c7ecd472194d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Aplicando GridSearchCV para Ridge...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'X_train' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-75179c79dbae>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;31m# Entrenar GridSearchCV en los datos de entrenamiento\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;31m# Esto realizará la validación cruzada para cada combinación de hiperparámetros\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m \u001b[0mgrid_search_ridge\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;31m# Obtener los mejores hiperparámetros encontrados\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
          ]
        }
      ],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import Ridge, LinearRegression\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, learning_curve\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "#-------------------Escoger modelo con ajuste de hyperparámetros con GridSearchCV y RandomizedSearchCV\n",
        "\n",
        "pipeline_ridge = Pipeline([\n",
        "    ('scaler', StandardScaler()), # Primer paso: escalar los datos\n",
        "    ('ridge', Ridge())           # Segundo paso: aplicar el modelo Ridge\n",
        "])\n",
        "\n",
        "# Definir el rango de hiperparámetros para GridSearchCV\n",
        "# Para Ridge, el hiperparámetro principal es 'alpha'\n",
        "# El nombre del parámetro en el pipeline es <nombre_del_paso>__<nombre_del_parametro>\n",
        "param_grid_ridge = {\n",
        "    'ridge__alpha': [0.001, 0.01, 0.1, 1, 10, 100] # Rango de valores de alpha a probar\n",
        "}\n",
        "\n",
        "# Definir el rango de hiperparámetros para RandomizedSearchCV\n",
        "# Puedes usar una distribución si el espacio de búsqueda es muy grande\n",
        "param_dist_ridge = {\n",
        "    'ridge__alpha': np.random.uniform(0.001, 100, 20) # 20 valores aleatorios entre 0.001 y 100\n",
        "}\n",
        "\n",
        "\n",
        "print(\"Aplicando GridSearchCV para Ridge...\")\n",
        "# Inicializar GridSearchCV\n",
        "# cv=5 significa validación cruzada de 5 pliegues\n",
        "# scoring='neg_mean_squared_error' se usa porque GridSearchCV por defecto maximiza el score,\n",
        "# y queremos minimizar el MSE, por lo tanto usamos el negativo.\n",
        "# n_jobs=-1 usa todos los núcleos disponibles\n",
        "grid_search_ridge = GridSearchCV(estimator=pipeline_ridge,\n",
        "                                 param_grid=param_grid_ridge,\n",
        "                                 cv=5,\n",
        "                                 scoring='neg_mean_squared_error',\n",
        "                                 n_jobs=-1)\n",
        "\n",
        "# Entrenar GridSearchCV en los datos de entrenamiento\n",
        "# Esto realizará la validación cruzada para cada combinación de hiperparámetros\n",
        "grid_search_ridge.fit(X_train, y_train)\n",
        "\n",
        "# Obtener los mejores hiperparámetros encontrados\n",
        "best_params_grid_ridge = grid_search_ridge.best_params_\n",
        "print(f\"Mejores hiperparámetros encontrados con GridSearchCV para Ridge: {best_params_grid_ridge}\")\n",
        "\n",
        "# Obtener el mejor modelo encontrado por GridSearchCV\n",
        "best_ridge_model_grid = grid_search_ridge.best_estimator_\n",
        "\n",
        "# Evaluar el mejor modelo en el conjunto de prueba\n",
        "y_pred_ridge_grid = best_ridge_model_grid.predict(X_test)\n",
        "rmse_ridge_grid = np.sqrt(mean_squared_error(y_test, y_pred_ridge_grid))\n",
        "r2_ridge_grid = r2_score(y_test, y_pred_ridge_grid)\n",
        "\n",
        "print(f\"Resultados del mejor modelo Ridge (GridSearchCV) en el conjunto de prueba:\")\n",
        "print(f\"  RMSE: {rmse_ridge_grid:.4f}\")\n",
        "print(f\"  R²: {r2_ridge_grid:.4f}\")\n",
        "\n",
        "# Graficar la comparación para el mejor modelo Ridge (GridSearchCV)\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(y_test, y_pred_ridge_grid, alpha=0.5, label='Observaciones (Valor Real vs Predicho)')\n",
        "plt.plot([y.min(), y.max()], [y.min(), y.max()], 'r--', lw=2, label='Línea Ideal (Real = Predicho)')\n",
        "plt.title(f'Valor real vs Valor predicho - Ridge (GridSearchCV)')\n",
        "plt.xlabel('Valor real')\n",
        "plt.ylabel('Valor predicho')\n",
        "plt.xlim(y.min(), y.max())\n",
        "plt.ylim(y.min(), y.max())\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "print(\"\\nAplicando RandomizedSearchCV para Ridge...\")\n",
        "# Inicializar RandomizedSearchCV\n",
        "# n_iter=20 significa que probará 20 combinaciones aleatorias\n",
        "# cv=5 significa validación cruzada de 5 pliegues\n",
        "# scoring='neg_mean_squared_error'\n",
        "# n_jobs=-1 usa todos los núcleos disponibles\n",
        "# random_state para reproducibilidad\n",
        "random_search_ridge = RandomizedSearchCV(estimator=pipeline_ridge,\n",
        "                                         param_distributions=param_dist_ridge,\n",
        "                                         n_iter=20, # Número de combinaciones a probar\n",
        "                                         cv=5,\n",
        "                                         scoring='neg_mean_squared_error',\n",
        "                                         n_jobs=-1,\n",
        "                                         random_state=42)\n",
        "\n",
        "# Entrenar RandomizedSearchCV en los datos de entrenamiento\n",
        "random_search_ridge.fit(X_train, y_train)\n",
        "\n",
        "# Obtener los mejores hiperparámetros encontrados\n",
        "best_params_random_ridge = random_search_ridge.best_params_\n",
        "print(f\"Mejores hiperparámetros encontrados con RandomizedSearchCV para Ridge: {best_params_random_ridge}\")\n",
        "\n",
        "# Obtener el mejor modelo encontrado por RandomizedSearchCV\n",
        "best_ridge_model_random = random_search_ridge.best_estimator_\n",
        "\n",
        "# Evaluar el mejor modelo en el conjunto de prueba\n",
        "y_pred_ridge_random = best_ridge_model_random.predict(X_test)\n",
        "rmse_ridge_random = np.sqrt(mean_squared_error(y_test, y_pred_ridge_random))\n",
        "r2_ridge_random = r2_score(y_test, y_pred_ridge_random)\n",
        "\n",
        "print(f\"Resultados del mejor modelo Ridge (RandomizedSearchCV) en el conjunto de prueba:\")\n",
        "print(f\"  RMSE: {rmse_ridge_random:.4f}\")\n",
        "print(f\"  R²: {r2_ridge_random:.4f}\")\n",
        "\n",
        "# Graficar la comparación para el mejor modelo Ridge (RandomizedSearchCV)\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(y_test, y_pred_ridge_random, alpha=0.5, label='Observaciones (Valor Real vs Predicho)')\n",
        "plt.plot([y.min(), y.max()], [y.min(), y.max()], 'r--', lw=2, label='Línea Ideal (Real = Predicho)')\n",
        "plt.title(f'Valor real vs Valor predicho - Ridge (RandomizedSearchCV)')\n",
        "plt.xlabel('Valor real')\n",
        "plt.ylabel('Valor predicho')\n",
        "plt.xlim(y.min(), y.max())\n",
        "plt.ylim(y.min(), y.max())\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "results['Ridge_GridSearchCV'] = {'RMSE': rmse_ridge_grid, 'R²': r2_ridge_grid}\n",
        "results['Ridge_RandomizedSearchCV'] = {'RMSE': rmse_ridge_random, 'R²': r2_ridge_random}\n",
        "print(\"\\nResumen de resultados:\")\n",
        "print(results)\n",
        "\n",
        "\n",
        "\n",
        "#-----------------------------Curvas de Aprendizaje Ridge-----------------------------\n",
        "X = data_social.drop('actual_productivity_score', axis=1).select_dtypes(include=np.number)\n",
        "y = data_social['actual_productivity_score']\n",
        "\n",
        "best_ridge_model = grid_search_ridge.best_estimator_\n",
        "linear_model = LinearRegression()\n",
        "\n",
        "models_for_learning_curve = {\n",
        "    'Ridge (Optimizado)': best_ridge_model\n",
        "}\n",
        "\n",
        "for name, estimator in models_for_learning_curve.items():\n",
        "    print(f\"Generando curva de aprendizaje para {name}...\")\n",
        "\n",
        "    # Calcula los puntos para la curva de aprendizaje\n",
        "    train_sizes, train_scores, test_scores = learning_curve(\n",
        "        estimator, X, y, cv=5, scoring='neg_mean_squared_error', n_jobs=-1,\n",
        "        train_sizes=np.linspace(0.1, 1.0, 10) # Genera 10 puntos entre 10% y 100% del tamaño del entrenamiento\n",
        "    )\n",
        "\n",
        "    # Convertir los scores de MSE negativo a RMSE (Raíz del Error Cuadrático Medio)\n",
        "    # RMSE = sqrt(-MSE)\n",
        "    train_scores_rmse = np.sqrt(-train_scores)\n",
        "    test_scores_rmse = np.sqrt(-test_scores)\n",
        "    # Calcular la media y desviación estándar de los scores para cada tamaño de entrenamiento\n",
        "    train_scores_mean = np.mean(train_scores_rmse, axis=1)\n",
        "    train_scores_std = np.std(train_scores_rmse, axis=1)\n",
        "    test_scores_mean = np.mean(test_scores_rmse, axis=1)\n",
        "    test_scores_std = np.std(test_scores_rmse, axis=1)\n",
        "\n",
        "    # Graficar la curva de aprendizaje\n",
        "    plt.figure(figsize=(5, 3))\n",
        "    plt.title(f\"Curva de Aprendizaje - {name}\")\n",
        "    plt.xlabel(\"Tamaño del Conjunto de Entrenamiento\")\n",
        "    plt.ylabel(\"RMSE\") # Usamos RMSE en el eje Y ya que es más interpretable\n",
        "\n",
        "    # Graficar la curva de entrenamiento con banda de error\n",
        "    plt.grid()\n",
        "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
        "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
        "                     color=\"r\")\n",
        "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
        "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
        "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
        "             label=\"Puntuación Entrenamiento\")\n",
        "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
        "             label=\"Puntuación Validación Cruzada\")\n",
        "\n",
        "    plt.legend(loc=\"best\")\n",
        "    plt.ylim(bottom=0) # Asegura que el eje Y comience en 0 o más\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "#Curva de aprendizaje implícita\n",
        "#El flujo sugiere una curva estable para Ridge:\n",
        "\n",
        "#Buen rendimiento en entrenamiento y prueba\n",
        "\n",
        "#Baja varianza entre distintos modelos\n",
        "#Conclusión: El modelo no está ni subajustado ni sobreajustado (overfitting ni underfitting)\n",
        "#y se encuentra en una zona óptima de la curva de aprendizaje. Esto indica un aprendizaje efectivo,\n",
        "# donde agregar más datos no necesariamente mejorará mucho el modelo sin cambiar su estructura.\n"
      ]
    }
  ]
}